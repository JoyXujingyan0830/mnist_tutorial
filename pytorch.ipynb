{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "learning_rate = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "test_x = torch.unsqueeze(test_dataset.test_data, dim=1).type(torch.FloatTensor)/255.\n",
    "test_y = test_dataset.test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class SimpleNet(nn.Module):\n",
    "# TODO:define model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(  # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,      # input height\n",
    "                out_channels=16,    # n_filters\n",
    "                kernel_size=5,      # filter size\n",
    "                stride=1,           # filter movement/step\n",
    "                padding=2,      #  padding=(kernel_size-1)/2 å½“ stride=1\n",
    "            ),      # output shape (16, 28, 28)\n",
    "            nn.ReLU(),    # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # sample in 2x2 space, output shape (16, 14, 14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(  # input shape (16, 14, 14)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.MaxPool2d(2),  # output shape (32, 7, 7)\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)   #  (batch_size, 32 * 7 * 7)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "cnn = CNN()\n",
    "#print(cnn)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss() \n",
    "\n",
    "    \n",
    "#model = SimpleNet()\n",
    "\n",
    "# TODO:define loss function and optimiter\n",
    "#criterion = \n",
    "#optimizer = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.3176 | train accuracy: 12.50% | test accuracy: 9.32%\n",
      "Epoch:  1 | train loss: 0.0523 | train accuracy: 98.44% | test accuracy: 79.59%\n",
      "Epoch:  2 | train loss: 0.0390 | train accuracy: 99.22% | test accuracy: 93.51%\n",
      "Epoch:  3 | train loss: 0.0109 | train accuracy: 100.00% | test accuracy: 87.18%\n",
      "Epoch:  4 | train loss: 0.0890 | train accuracy: 98.44% | test accuracy: 95.26%\n",
      "Epoch:  5 | train loss: 0.0396 | train accuracy: 99.22% | test accuracy: 92.39%\n",
      "Epoch:  6 | train loss: 0.0562 | train accuracy: 99.22% | test accuracy: 84.72%\n",
      "Epoch:  7 | train loss: 0.0529 | train accuracy: 99.22% | test accuracy: 89.26%\n",
      "Epoch:  8 | train loss: 0.0606 | train accuracy: 98.44% | test accuracy: 85.60%\n",
      "Epoch:  9 | train loss: 0.2048 | train accuracy: 97.66% | test accuracy: 82.08%\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "   # for images, labels in tqdm(train_loader):\n",
    "        # TODO:forward + backward + optimize\n",
    "        \n",
    "        for step, (x, y) in enumerate(train_loader):   # distribute batch data, normalize x when iterate train_loader\n",
    "            b_x =Variable(x)\n",
    "            b_y =Variable(y)\n",
    "\n",
    "            output = cnn(b_x)\n",
    "            loss = loss_func(output,b_y)   # cross entropy loss\n",
    "\n",
    "            optimizer.zero_grad()           # clear gradients for this training step/usr/bin\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            optimizer.step()                # apply gradients\n",
    "\n",
    "            if step % 512 == 0:\n",
    "                test_output = cnn (test_x)  \n",
    "                testpred_y = torch.max (test_output, 1)[1].data.squeeze ()\n",
    "                train_output = cnn (x)\n",
    "                trainpred_y = torch.max (train_output, 1)[1].data.squeeze ()\n",
    "                testaccuracy = (testpred_y == test_y).sum().numpy() / test_y.size(0)\n",
    "                trainaccuracy = (trainpred_y == y).sum ().numpy () / y.size (0)\n",
    "                print ('Epoch: ', epoch, '| train loss: %.4f' % loss.item(), '| train accuracy: %.2f%%' % (trainaccuracy*100) ,'| test accuracy: %.2f%%' % (testaccuracy*100))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print (\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
